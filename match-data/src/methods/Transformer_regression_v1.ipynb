{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        datas = json.load(file)\n",
    "\n",
    "    dataset = []\n",
    "    for data in datas:\n",
    "        user_info = []\n",
    "        for key, value in data[\"user_info\"].items():\n",
    "            if value:\n",
    "                user_info.append(f\"{key}_{value}\")\n",
    "        \n",
    "        parameter = []\n",
    "        for key, value in data[\"parameter\"].items():\n",
    "            parameter.append(value)\n",
    "        dataset.append({\"user_info\": user_info, \"parameter\": parameter})\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_info': ['age_81',\n",
       "  'r_htl_8000_80',\n",
       "  'r_htl_6000_70',\n",
       "  'r_htl_4000_60',\n",
       "  'r_htl_1000_70',\n",
       "  'r_htl_2000_60',\n",
       "  'r_htl_500_70',\n",
       "  'l_htl_8000_85',\n",
       "  'l_htl_4000_85',\n",
       "  'l_htl_1000_100',\n",
       "  'l_htl_2000_85',\n",
       "  'l_htl_500_70'],\n",
       " 'parameter': [2,\n",
       "  47,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  48,\n",
       "  38,\n",
       "  39,\n",
       "  35,\n",
       "  31,\n",
       "  30,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  60,\n",
       "  43,\n",
       "  43,\n",
       "  43,\n",
       "  43,\n",
       "  43,\n",
       "  43,\n",
       "  61,\n",
       "  51,\n",
       "  52,\n",
       "  48,\n",
       "  44,\n",
       "  45,\n",
       "  43,\n",
       "  43,\n",
       "  43,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  15,\n",
       "  0,\n",
       "  47,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  48,\n",
       "  38,\n",
       "  35,\n",
       "  30,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  60,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  61,\n",
       "  51,\n",
       "  52,\n",
       "  48,\n",
       "  44,\n",
       "  43,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  15,\n",
       "  0,\n",
       "  47,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  48,\n",
       "  38,\n",
       "  35,\n",
       "  30,\n",
       "  28,\n",
       "  28,\n",
       "  28,\n",
       "  60,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  61,\n",
       "  51,\n",
       "  52,\n",
       "  48,\n",
       "  44,\n",
       "  43,\n",
       "  41,\n",
       "  41,\n",
       "  41,\n",
       "  60,\n",
       "  70,\n",
       "  7,\n",
       "  10,\n",
       "  3,\n",
       "  5]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../../train_data/type_1/data.json\"\n",
    "dataset = process_data(data_path)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1214"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special symbols\n",
    "special_symbols = ['<pad>',]\n",
    "\n",
    "# make vocab\n",
    "user_info_vocab = list(special_symbols)\n",
    "\n",
    "for data in dataset:\n",
    "    user_info_vocab.extend(data[\"user_info\"])\n",
    "\n",
    "user_info_vocab = sorted(list(set(user_info_vocab)))\n",
    "\n",
    "len(user_info_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: <pad> —— Index: 0\n",
      "Word: age_1 —— Index: 1\n",
      "Word: age_10 —— Index: 2\n",
      "Word: age_104 —— Index: 3\n",
      "Word: age_1048 —— Index: 4\n",
      "Word: age_11 —— Index: 5\n",
      "Word: age_12 —— Index: 6\n",
      "Word: age_13 —— Index: 7\n",
      "Word: age_14 —— Index: 8\n",
      "Word: age_15 —— Index: 9\n",
      "Word: age_16 —— Index: 10\n",
      "Word: age_17 —— Index: 11\n",
      "Word: age_18 —— Index: 12\n",
      "Word: age_19 —— Index: 13\n",
      "Word: age_2 —— Index: 14\n",
      "Word: age_20 —— Index: 15\n",
      "Word: age_21 —— Index: 16\n",
      "Word: age_22 —— Index: 17\n",
      "Word: age_23 —— Index: 18\n",
      "Word: age_24 —— Index: 19\n",
      "Word: age_25 —— Index: 20\n",
      "Word: age_26 —— Index: 21\n",
      "Word: age_27 —— Index: 22\n",
      "Word: age_28 —— Index: 23\n",
      "Word: age_29 —— Index: 24\n",
      "Word: age_3 —— Index: 25\n",
      "Word: age_30 —— Index: 26\n",
      "Word: age_31 —— Index: 27\n",
      "Word: age_32 —— Index: 28\n",
      "Word: age_33 —— Index: 29\n",
      "Word: age_34 —— Index: 30\n",
      "Word: age_35 —— Index: 31\n",
      "Word: age_36 —— Index: 32\n",
      "Word: age_37 —— Index: 33\n",
      "Word: age_38 —— Index: 34\n",
      "Word: age_39 —— Index: 35\n",
      "Word: age_4 —— Index: 36\n",
      "Word: age_40 —— Index: 37\n",
      "Word: age_41 —— Index: 38\n",
      "Word: age_42 —— Index: 39\n",
      "Word: age_43 —— Index: 40\n",
      "Word: age_44 —— Index: 41\n",
      "Word: age_45 —— Index: 42\n",
      "Word: age_46 —— Index: 43\n",
      "Word: age_47 —— Index: 44\n",
      "Word: age_48 —— Index: 45\n",
      "Word: age_49 —— Index: 46\n",
      "Word: age_5 —— Index: 47\n",
      "Word: age_50 —— Index: 48\n",
      "Word: age_51 —— Index: 49\n",
      "Word: age_52 —— Index: 50\n",
      "Word: age_53 —— Index: 51\n",
      "Word: age_54 —— Index: 52\n",
      "Word: age_55 —— Index: 53\n",
      "Word: age_56 —— Index: 54\n",
      "Word: age_57 —— Index: 55\n",
      "Word: age_58 —— Index: 56\n",
      "Word: age_59 —— Index: 57\n",
      "Word: age_6 —— Index: 58\n",
      "Word: age_60 —— Index: 59\n",
      "Word: age_61 —— Index: 60\n",
      "Word: age_62 —— Index: 61\n",
      "Word: age_63 —— Index: 62\n",
      "Word: age_64 —— Index: 63\n",
      "Word: age_65 —— Index: 64\n",
      "Word: age_66 —— Index: 65\n",
      "Word: age_67 —— Index: 66\n",
      "Word: age_68 —— Index: 67\n",
      "Word: age_69 —— Index: 68\n",
      "Word: age_7 —— Index: 69\n",
      "Word: age_70 —— Index: 70\n",
      "Word: age_71 —— Index: 71\n",
      "Word: age_72 —— Index: 72\n",
      "Word: age_73 —— Index: 73\n",
      "Word: age_74 —— Index: 74\n",
      "Word: age_75 —— Index: 75\n",
      "Word: age_76 —— Index: 76\n",
      "Word: age_77 —— Index: 77\n",
      "Word: age_78 —— Index: 78\n",
      "Word: age_79 —— Index: 79\n",
      "Word: age_8 —— Index: 80\n",
      "Word: age_80 —— Index: 81\n",
      "Word: age_81 —— Index: 82\n",
      "Word: age_82 —— Index: 83\n",
      "Word: age_83 —— Index: 84\n",
      "Word: age_84 —— Index: 85\n",
      "Word: age_85 —— Index: 86\n",
      "Word: age_86 —— Index: 87\n",
      "Word: age_87 —— Index: 88\n",
      "Word: age_88 —— Index: 89\n",
      "Word: age_89 —— Index: 90\n",
      "Word: age_9 —— Index: 91\n",
      "Word: age_90 —— Index: 92\n",
      "Word: age_91 —— Index: 93\n",
      "Word: age_92 —— Index: 94\n",
      "Word: age_93 —— Index: 95\n",
      "Word: age_94 —— Index: 96\n",
      "Word: age_95 —— Index: 97\n",
      "Word: age_96 —— Index: 98\n",
      "Word: age_97 —— Index: 99\n",
      "Word: age_98 —— Index: 100\n",
      "Word: l_bcl_1000_-10 —— Index: 101\n",
      "Word: l_bcl_1000_-5 —— Index: 102\n",
      "Word: l_bcl_1000_10 —— Index: 103\n",
      "Word: l_bcl_1000_100 —— Index: 104\n",
      "Word: l_bcl_1000_105 —— Index: 105\n",
      "Word: l_bcl_1000_110 —— Index: 106\n",
      "Word: l_bcl_1000_120 —— Index: 107\n",
      "Word: l_bcl_1000_130 —— Index: 108\n",
      "Word: l_bcl_1000_15 —— Index: 109\n",
      "Word: l_bcl_1000_20 —— Index: 110\n",
      "Word: l_bcl_1000_25 —— Index: 111\n",
      "Word: l_bcl_1000_30 —— Index: 112\n",
      "Word: l_bcl_1000_35 —— Index: 113\n",
      "Word: l_bcl_1000_40 —— Index: 114\n",
      "Word: l_bcl_1000_45 —— Index: 115\n",
      "Word: l_bcl_1000_475 —— Index: 116\n",
      "Word: l_bcl_1000_5 —— Index: 117\n",
      "Word: l_bcl_1000_50 —— Index: 118\n",
      "Word: l_bcl_1000_55 —— Index: 119\n",
      "Word: l_bcl_1000_60 —— Index: 120\n",
      "Word: l_bcl_1000_65 —— Index: 121\n",
      "Word: l_bcl_1000_70 —— Index: 122\n",
      "Word: l_bcl_1000_75 —— Index: 123\n",
      "Word: l_bcl_1000_80 —— Index: 124\n",
      "Word: l_bcl_1000_85 —— Index: 125\n",
      "Word: l_bcl_1000_90 —— Index: 126\n",
      "Word: l_bcl_1000_95 —— Index: 127\n",
      "Word: l_bcl_1500_10 —— Index: 128\n",
      "Word: l_bcl_1500_15 —— Index: 129\n",
      "Word: l_bcl_1500_25 —— Index: 130\n",
      "Word: l_bcl_1500_30 —— Index: 131\n",
      "Word: l_bcl_1500_35 —— Index: 132\n",
      "Word: l_bcl_1500_40 —— Index: 133\n",
      "Word: l_bcl_1500_45 —— Index: 134\n",
      "Word: l_bcl_1500_50 —— Index: 135\n",
      "Word: l_bcl_1500_55 —— Index: 136\n",
      "Word: l_bcl_1500_60 —— Index: 137\n",
      "Word: l_bcl_1500_65 —— Index: 138\n",
      "Word: l_bcl_1500_70 —— Index: 139\n",
      "Word: l_bcl_1500_75 —— Index: 140\n",
      "Word: l_bcl_1500_85 —— Index: 141\n",
      "Word: l_bcl_2000_-5 —— Index: 142\n",
      "Word: l_bcl_2000_10 —— Index: 143\n",
      "Word: l_bcl_2000_100 —— Index: 144\n",
      "Word: l_bcl_2000_105 —— Index: 145\n",
      "Word: l_bcl_2000_110 —— Index: 146\n",
      "Word: l_bcl_2000_120 —— Index: 147\n",
      "Word: l_bcl_2000_130 —— Index: 148\n",
      "Word: l_bcl_2000_15 —— Index: 149\n",
      "Word: l_bcl_2000_20 —— Index: 150\n",
      "Word: l_bcl_2000_25 —— Index: 151\n",
      "Word: l_bcl_2000_30 —— Index: 152\n",
      "Word: l_bcl_2000_35 —— Index: 153\n",
      "Word: l_bcl_2000_40 —— Index: 154\n",
      "Word: l_bcl_2000_45 —— Index: 155\n",
      "Word: l_bcl_2000_5 —— Index: 156\n",
      "Word: l_bcl_2000_50 —— Index: 157\n",
      "Word: l_bcl_2000_55 —— Index: 158\n",
      "Word: l_bcl_2000_60 —— Index: 159\n",
      "Word: l_bcl_2000_65 —— Index: 160\n",
      "Word: l_bcl_2000_70 —— Index: 161\n",
      "Word: l_bcl_2000_75 —— Index: 162\n",
      "Word: l_bcl_2000_80 —— Index: 163\n",
      "Word: l_bcl_2000_85 —— Index: 164\n",
      "Word: l_bcl_2000_90 —— Index: 165\n",
      "Word: l_bcl_2000_95 —— Index: 166\n",
      "Word: l_bcl_250_-10 —— Index: 167\n",
      "Word: l_bcl_250_-5 —— Index: 168\n",
      "Word: l_bcl_250_10 —— Index: 169\n",
      "Word: l_bcl_250_100 —— Index: 170\n",
      "Word: l_bcl_250_110 —— Index: 171\n",
      "Word: l_bcl_250_120 —— Index: 172\n",
      "Word: l_bcl_250_130 —— Index: 173\n",
      "Word: l_bcl_250_15 —— Index: 174\n",
      "Word: l_bcl_250_20 —— Index: 175\n",
      "Word: l_bcl_250_25 —— Index: 176\n",
      "Word: l_bcl_250_30 —— Index: 177\n",
      "Word: l_bcl_250_35 —— Index: 178\n",
      "Word: l_bcl_250_40 —— Index: 179\n",
      "Word: l_bcl_250_45 —— Index: 180\n",
      "Word: l_bcl_250_5 —— Index: 181\n",
      "Word: l_bcl_250_50 —— Index: 182\n",
      "Word: l_bcl_250_55 —— Index: 183\n",
      "Word: l_bcl_250_60 —— Index: 184\n",
      "Word: l_bcl_250_65 —— Index: 185\n",
      "Word: l_bcl_250_70 —— Index: 186\n",
      "Word: l_bcl_250_75 —— Index: 187\n",
      "Word: l_bcl_250_80 —— Index: 188\n",
      "Word: l_bcl_250_85 —— Index: 189\n",
      "Word: l_bcl_250_90 —— Index: 190\n",
      "Word: l_bcl_250_95 —— Index: 191\n",
      "Word: l_bcl_3000_20 —— Index: 192\n",
      "Word: l_bcl_3000_25 —— Index: 193\n",
      "Word: l_bcl_3000_30 —— Index: 194\n",
      "Word: l_bcl_3000_35 —— Index: 195\n",
      "Word: l_bcl_3000_40 —— Index: 196\n",
      "Word: l_bcl_3000_45 —— Index: 197\n",
      "Word: l_bcl_3000_50 —— Index: 198\n",
      "Word: l_bcl_3000_55 —— Index: 199\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(user_info_vocab):\n",
    "    if index < 200:\n",
    "        print(f\"Word: {word} —— Index: {index}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../../train_data/type_1/train.json\"\n",
    "valid_path = \"../../train_data/type_1/valid.json\"\n",
    "\n",
    "train_dataset = process_data(train_path)\n",
    "valid_dataset = process_data(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize (only input data)\n",
    "token_train_dataset = []\n",
    "for data in train_dataset:\n",
    "    user_info = [user_info_vocab.index(word) for word in data[\"user_info\"]]\n",
    "    token_train_dataset.append({\"user_info\": user_info, \"parameter\": data[\"parameter\"]})\n",
    "\n",
    "\n",
    "token_valid_dataset = []\n",
    "for data in valid_dataset:\n",
    "    user_info = [user_info_vocab.index(word) for word in data[\"user_info\"]]\n",
    "    token_valid_dataset.append({\"user_info\": user_info, \"parameter\": data[\"parameter\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 1096, 1003, 886, 930, 953, 1026, 445, 330, 373, 401, 471]\n",
      "[11, 39, 38, 38, 38, 38, 38, 38, 41, 39, 39, 42, 43, 42, 38, 38, 38, 57, 56, 56, 56, 56, 56, 56, 58, 58, 57, 59, 60, 60, 56, 56, 56, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 15, 8, 40, 37, 37, 37, 37, 37, 37, 38, 40, 33, 33, 37, 37, 37, 54, 51, 51, 51, 51, 51, 51, 52, 54, 52, 47, 58, 47, 51, 51, 51, 15, 4, 40, 37, 37, 37, 37, 37, 37, 38, 40, 33, 33, 37, 37, 37, 54, 51, 51, 51, 51, 51, 51, 52, 54, 52, 47, 58, 47, 51, 51, 51, 60, 70, 7, 10, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "for data in token_train_dataset:\n",
    "    print(data[\"user_info\"])\n",
    "    print(data[\"parameter\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HearingAidDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "        # 提前将参数归一化\n",
    "        params = [item[\"parameter\"] for item in dataset]\n",
    "        self.scaled_params = self.scaler.fit_transform(params)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_info = self.dataset[idx][\"user_info\"]\n",
    "        user_info_tensor = torch.tensor(user_info, dtype=torch.long)\n",
    "        param_tensor = torch.tensor(self.scaled_params[idx], dtype=torch.float32)\n",
    "        \n",
    "        return user_info_tensor, param_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=300, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # Shape (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            x: (batch_size, x_len, d_model)\n",
    "            requires_grad_(False) is used to prevent the model from updating the positional encoding\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMultiOutputRegressor(nn.Module):\n",
    "    def __init__(self, src_vocab_size, n_outputs, d_model, n_heads, n_layers, dropout=0.1, batch_first=True):\n",
    "        super().__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=n_heads, \n",
    "            num_encoder_layers=n_layers, \n",
    "            num_decoder_layers=n_layers,\n",
    "            dropout=dropout, \n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "        self.regressor = nn.Linear(d_model, n_outputs)\n",
    "\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_key_padding_mask = (src == 2)\n",
    "\n",
    "        src = self.src_tok_emb(src)\n",
    "        src = self.positional_encoding(src)\n",
    "\n",
    "        memory = self.transformer.encoder(\n",
    "            src,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        output = self.regressor(memory[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_train_dataset[0][\"parameter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerMultiOutputRegressor(\n",
       "  (src_tok_emb): Embedding(1214, 128)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (regressor): Linear(in_features=128, out_features=119, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化模型\n",
    "src_vocab_size = len(user_info_vocab)\n",
    "n_outputs = len(token_train_dataset[0][\"parameter\"])\n",
    "d_model = 128\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "dropout = 0.1\n",
    "batch_first = True\n",
    "\n",
    "model = TransformerMultiOutputRegressor(src_vocab_size, n_outputs, d_model, n_heads, n_layers, dropout, batch_first).to(DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    user_info, param = zip(*batch)\n",
    "    user_info = nn.utils.rnn.pad_sequence(user_info, batch_first=True, padding_value=user_info_vocab.index('<pad>'))\n",
    "\n",
    "    param = torch.stack(param)\n",
    "    return user_info, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = HearingAidDataset(token_train_dataset)\n",
    "valid_dataset = HearingAidDataset(token_valid_dataset)\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  78,    0,    0,  ...,    0,    0,    0],\n",
      "        [  72, 1091,  995,  ...,    0,    0,    0],\n",
      "        [  68, 1096, 1000,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  55, 1074,  979,  ...,    0,    0,    0],\n",
      "        [  66, 1089,  995,  ...,    0,    0,    0],\n",
      "        [  61, 1092, 1000,  ...,    0,    0,    0]], device='cuda:0')\n",
      "torch.Size([64, 119])\n",
      "torch.Size([64, 119])\n"
     ]
    }
   ],
   "source": [
    "# 测试模型输出\n",
    "for user_info, param in training_loader:\n",
    "    user_info = user_info.to(DEVICE)\n",
    "    param = param.to(DEVICE)\n",
    "    print(user_info)\n",
    "    print(param.shape)\n",
    "    output = model(user_info)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.208, Validation Loss: 0.138\n",
      "Epoch 2, Training Loss: 0.147, Validation Loss: 0.139\n",
      "Epoch 3, Training Loss: 0.139, Validation Loss: 0.138\n",
      "Epoch 4, Training Loss: 0.134, Validation Loss: 0.138\n",
      "Epoch 5, Training Loss: 0.132, Validation Loss: 0.139\n",
      "Epoch 6, Training Loss: 0.130, Validation Loss: 0.138\n",
      "Epoch 7, Training Loss: 0.129, Validation Loss: 0.138\n",
      "Epoch 8, Training Loss: 0.127, Validation Loss: 0.139\n",
      "Epoch 9, Training Loss: 0.127, Validation Loss: 0.137\n",
      "Epoch 10, Training Loss: 0.126, Validation Loss: 0.137\n",
      "Epoch 11, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 12, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 13, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 14, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 15, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 16, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 17, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 18, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 19, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Epoch 20, Training Loss: 0.125, Validation Loss: 0.137\n",
      "Early stopping triggered\n",
      "Training complete. Best Validation Loss: 0.137\n"
     ]
    }
   ],
   "source": [
    "# Model, loss function, optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1) \n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Example training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for user_info, param in training_loader:\n",
    "        user_info = user_info.to(DEVICE)\n",
    "        param = param.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user_info)\n",
    "        loss = criterion(output, param)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(training_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for user_info, param in valid_loader:\n",
    "            user_info = user_info.to(DEVICE)\n",
    "            param = param.to(DEVICE)\n",
    "            output = model(user_info)\n",
    "            val_loss = criterion(output, param)\n",
    "            total_val_loss += val_loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    valid_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.3f}, Validation Loss: {avg_val_loss:.3f}\")\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '../models/best_model_type_2.pth')  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "    \n",
    "    scheduler.step()  # Update learning rate\n",
    "\n",
    "print(\"Training complete. Best Validation Loss: {:.3f}\".format(best_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerMultiOutputRegressor(\n",
       "  (src_tok_emb): Embedding(1214, 128)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (regressor): Linear(in_features=128, out_features=119, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../models/best_model_type_1.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample):\n",
    "    # Tokenize the sample\n",
    "    user_info = [user_info_vocab.index(word) for word in sample[\"user_info\"]]\n",
    "    real = sample[\"parameter\"]\n",
    "    scaler = MinMaxScaler()\n",
    "    real_normalized = scaler.fit_transform([real])\n",
    "\n",
    "    # Convert to tensor\n",
    "    user_info_tensor = torch.tensor(user_info, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    user_info_tensor\n",
    "\n",
    "    # Make a prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(user_info_tensor)\n",
    "        pred = scaler.inverse_transform(pred.cpu().numpy())[0]\n",
    "        # pred = [int(i) for i in pred]\n",
    "   \n",
    "    return real, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>真实值</th>\n",
       "      <th>预测值</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.156035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>33.432240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>21.347393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>21.347170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>21.348337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>21.348183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>21.350311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21</td>\n",
       "      <td>21.349110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>36.473633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36</td>\n",
       "      <td>36.493484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>43</td>\n",
       "      <td>43.507168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>39</td>\n",
       "      <td>39.449551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34</td>\n",
       "      <td>34.405804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26</td>\n",
       "      <td>26.348177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21</td>\n",
       "      <td>21.346899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21</td>\n",
       "      <td>21.348043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21</td>\n",
       "      <td>21.349909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47</td>\n",
       "      <td>47.606407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>28</td>\n",
       "      <td>28.517878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>28.519196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28</td>\n",
       "      <td>28.519287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>28</td>\n",
       "      <td>28.515369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>28</td>\n",
       "      <td>28.522635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>28</td>\n",
       "      <td>28.519402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>49</td>\n",
       "      <td>49.648144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50</td>\n",
       "      <td>50.598061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50</td>\n",
       "      <td>50.609875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>46</td>\n",
       "      <td>46.597813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41</td>\n",
       "      <td>41.528008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>33</td>\n",
       "      <td>33.440353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>28</td>\n",
       "      <td>28.509953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>28</td>\n",
       "      <td>28.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>28</td>\n",
       "      <td>28.513550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>34.648777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>34.644753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>34.649525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>34</td>\n",
       "      <td>34.634281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>34</td>\n",
       "      <td>34.640251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>34</td>\n",
       "      <td>34.635372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>34</td>\n",
       "      <td>34.644714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>34</td>\n",
       "      <td>34.607540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>34</td>\n",
       "      <td>34.601223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>34</td>\n",
       "      <td>34.608551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>34</td>\n",
       "      <td>34.605083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>34</td>\n",
       "      <td>34.605583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>34</td>\n",
       "      <td>34.623318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>34</td>\n",
       "      <td>34.613167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>34</td>\n",
       "      <td>34.620987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>34</td>\n",
       "      <td>34.619331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>15</td>\n",
       "      <td>15.001520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4</td>\n",
       "      <td>4.092849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>42</td>\n",
       "      <td>42.496029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>23</td>\n",
       "      <td>23.358988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>23</td>\n",
       "      <td>23.361345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>23</td>\n",
       "      <td>23.367153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>23</td>\n",
       "      <td>23.361269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>23</td>\n",
       "      <td>23.365396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>23</td>\n",
       "      <td>23.364069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>44</td>\n",
       "      <td>44.548775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>45</td>\n",
       "      <td>45.522991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>41</td>\n",
       "      <td>41.450150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>28</td>\n",
       "      <td>28.357716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>23</td>\n",
       "      <td>23.352293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>23</td>\n",
       "      <td>23.358532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>23</td>\n",
       "      <td>23.352194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>47</td>\n",
       "      <td>47.608501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>28</td>\n",
       "      <td>28.527212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>28</td>\n",
       "      <td>28.521292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>28</td>\n",
       "      <td>28.522577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>28</td>\n",
       "      <td>28.529495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>28</td>\n",
       "      <td>28.522604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>28</td>\n",
       "      <td>28.526779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>49</td>\n",
       "      <td>49.636917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>50</td>\n",
       "      <td>50.595680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>50</td>\n",
       "      <td>50.601887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>46</td>\n",
       "      <td>46.539627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>41</td>\n",
       "      <td>41.531769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>33</td>\n",
       "      <td>33.468300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>28</td>\n",
       "      <td>28.465492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>28</td>\n",
       "      <td>28.468323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>28</td>\n",
       "      <td>28.473925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>15</td>\n",
       "      <td>15.005847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>42</td>\n",
       "      <td>42.495506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>23</td>\n",
       "      <td>23.360088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>23</td>\n",
       "      <td>23.361622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>23</td>\n",
       "      <td>23.359367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>23</td>\n",
       "      <td>23.361057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>23</td>\n",
       "      <td>23.361942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>23</td>\n",
       "      <td>23.360140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>44</td>\n",
       "      <td>44.546806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>45</td>\n",
       "      <td>45.525410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>41</td>\n",
       "      <td>41.442955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>28</td>\n",
       "      <td>28.365248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>23</td>\n",
       "      <td>23.360266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>23</td>\n",
       "      <td>23.356565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>23</td>\n",
       "      <td>23.357286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>47</td>\n",
       "      <td>47.613873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>28</td>\n",
       "      <td>28.522936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>28</td>\n",
       "      <td>28.524658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>28</td>\n",
       "      <td>28.519321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>28</td>\n",
       "      <td>28.525770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>28</td>\n",
       "      <td>28.528343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>28</td>\n",
       "      <td>28.527899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>49</td>\n",
       "      <td>49.639915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>50</td>\n",
       "      <td>50.596138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>50</td>\n",
       "      <td>50.605511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>46</td>\n",
       "      <td>46.542156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>41</td>\n",
       "      <td>41.542198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>33</td>\n",
       "      <td>33.474842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>28</td>\n",
       "      <td>28.464659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>28</td>\n",
       "      <td>28.470425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>28</td>\n",
       "      <td>28.472412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>60</td>\n",
       "      <td>60.664860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>70</td>\n",
       "      <td>70.849258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>7</td>\n",
       "      <td>7.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>10</td>\n",
       "      <td>10.177794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>3</td>\n",
       "      <td>3.819165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>3</td>\n",
       "      <td>3.207192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     真实值        预测值\n",
       "0      0   0.156035\n",
       "1     33  33.432240\n",
       "2     21  21.347393\n",
       "3     21  21.347170\n",
       "4     21  21.348337\n",
       "5     21  21.348183\n",
       "6     21  21.350311\n",
       "7     21  21.349110\n",
       "8     36  36.473633\n",
       "9     36  36.493484\n",
       "10    43  43.507168\n",
       "11    39  39.449551\n",
       "12    34  34.405804\n",
       "13    26  26.348177\n",
       "14    21  21.346899\n",
       "15    21  21.348043\n",
       "16    21  21.349909\n",
       "17    47  47.606407\n",
       "18    28  28.517878\n",
       "19    28  28.519196\n",
       "20    28  28.519287\n",
       "21    28  28.515369\n",
       "22    28  28.522635\n",
       "23    28  28.519402\n",
       "24    49  49.648144\n",
       "25    50  50.598061\n",
       "26    50  50.609875\n",
       "27    46  46.597813\n",
       "28    41  41.528008\n",
       "29    33  33.440353\n",
       "30    28  28.509953\n",
       "31    28  28.510000\n",
       "32    28  28.513550\n",
       "33    34  34.648777\n",
       "34    34  34.644753\n",
       "35    34  34.649525\n",
       "36    34  34.634281\n",
       "37    34  34.640251\n",
       "38    34  34.635372\n",
       "39    34  34.644714\n",
       "40    34  34.607540\n",
       "41    34  34.601223\n",
       "42    34  34.608551\n",
       "43    34  34.605083\n",
       "44    34  34.605583\n",
       "45    34  34.623318\n",
       "46    34  34.613167\n",
       "47    34  34.620987\n",
       "48    34  34.619331\n",
       "49    15  15.001520\n",
       "50     4   4.092849\n",
       "51    42  42.496029\n",
       "52    23  23.358988\n",
       "53    23  23.361345\n",
       "54    23  23.367153\n",
       "55    23  23.361269\n",
       "56    23  23.365396\n",
       "57    23  23.364069\n",
       "58    44  44.548775\n",
       "59    45  45.522991\n",
       "60    41  41.450150\n",
       "61    28  28.357716\n",
       "62    23  23.352293\n",
       "63    23  23.358532\n",
       "64    23  23.352194\n",
       "65    47  47.608501\n",
       "66    28  28.527212\n",
       "67    28  28.521292\n",
       "68    28  28.522577\n",
       "69    28  28.529495\n",
       "70    28  28.522604\n",
       "71    28  28.526779\n",
       "72    49  49.636917\n",
       "73    50  50.595680\n",
       "74    50  50.601887\n",
       "75    46  46.539627\n",
       "76    41  41.531769\n",
       "77    33  33.468300\n",
       "78    28  28.465492\n",
       "79    28  28.468323\n",
       "80    28  28.473925\n",
       "81    15  15.005847\n",
       "82     0   0.001119\n",
       "83    42  42.495506\n",
       "84    23  23.360088\n",
       "85    23  23.361622\n",
       "86    23  23.359367\n",
       "87    23  23.361057\n",
       "88    23  23.361942\n",
       "89    23  23.360140\n",
       "90    44  44.546806\n",
       "91    45  45.525410\n",
       "92    41  41.442955\n",
       "93    28  28.365248\n",
       "94    23  23.360266\n",
       "95    23  23.356565\n",
       "96    23  23.357286\n",
       "97    47  47.613873\n",
       "98    28  28.522936\n",
       "99    28  28.524658\n",
       "100   28  28.519321\n",
       "101   28  28.525770\n",
       "102   28  28.528343\n",
       "103   28  28.527899\n",
       "104   49  49.639915\n",
       "105   50  50.596138\n",
       "106   50  50.605511\n",
       "107   46  46.542156\n",
       "108   41  41.542198\n",
       "109   33  33.474842\n",
       "110   28  28.464659\n",
       "111   28  28.470425\n",
       "112   28  28.472412\n",
       "113   60  60.664860\n",
       "114   70  70.849258\n",
       "115    7   7.808200\n",
       "116   10  10.177794\n",
       "117    3   3.819165\n",
       "118    3   3.207192"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# randomly select a sample from the test set\n",
    "test_path = \"../../train_data/type_1/test.json\"\n",
    "test_dataset = process_data(test_path)\n",
    "\n",
    "sample = random.choice(test_dataset)\n",
    "real, pred = predict(sample)\n",
    "\n",
    "# calculate the accuracy\n",
    "correct = 0\n",
    "for a, b in zip(real, pred):\n",
    "    if  abs(a - b) <= 0:\n",
    "        correct += 1\n",
    "print(f\"Accuracy: {correct / len(real) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    '真实值': real,\n",
    "    '预测值': pred,\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 计算误差\n",
    "# pred = np.array(pred)\n",
    "# real = np.array(real)\n",
    "# errors = pred - real\n",
    "\n",
    "# # 绘制误差的箱型图\n",
    "# plt.boxplot(errors)\n",
    "# plt.title(\"Prediction Errors Boxplot\")\n",
    "# plt.xlabel(\"Error\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for item in test_dataset:\n",
    "    if item[\"user_info\"]:\n",
    "        reals, preds = predict(item)\n",
    "        correct = 0\n",
    "        for real, pred in zip(reals, preds):\n",
    "            if abs(real - pred) <= 1:\n",
    "                correct += 1\n",
    "        accuracy.append(correct / len(reals))\n",
    "\n",
    "print(f\"Average Accuracy: {sum(accuracy) / len(accuracy) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
